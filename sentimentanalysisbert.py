# -*- coding: utf-8 -*-
"""SentimentAnalysisBERT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sSmSx8z7GLQAR1LhBJeC66nDHH5OpJ9W

**STEP 01: Importing and Installing Dependencies**
"""

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117

!pip install transformers requests beautifulsoup4 numpy pandas

import torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification

import requests
import re
from bs4 import BeautifulSoup

"""**STEP 02: Instantiating**"""

tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

"""**STEP 03: Encoding and Sentiment calculation**"""

tokens = tokenizer.encode('No, i do not want to nove forward with it', return_tensors='pt')

"""here the tokenizer basically encodes the individual strings into numbers and then creates it as a tensor in pytorch hence 'pt'.
we can also try the reverse by decoding the tokens variable to convert nos back to string
"""

tokens

"""displaying the tokens"""

result = model(tokens)
result

"""here we pass the tokens list into our model which analyses the variables and does its magic and then returns the values of sentiments; 
the values(scores) in the logits section is the probability of that word being the sentiment conveyed in the statement; the index position with the highest socre represents the sentiment of the overall sentence. ; also note that since it is an array positions are from 0-4 but as sentiments are from 1 -5 we need to incement later
"""

result.logits

int(torch.argmax(result.logits))+1 # finds the maximum value's index position in the array of tensors and increments by 1

"""**STEP 03: Web Scraping**"""

r = requests.get('https://www.yelp.com/biz/mejico-sydney-2')
# this uses the requests  library to fetch the webpage specified
r.text 
# this method translates all the html code of the webpage into text format

soup = BeautifulSoup(r.text , 'html.parser')
# this passes the text of the webpage and applies a html parser to it using bs4 and stores in soup

regex = re.compile('.*comment.*') # initialises the regex variable to comment tags in the webpage code
results = soup.find_all('p',{'class':regex}) # the results variable stores stores the output of; applying find_all method to soup variable ;
                                             # find_all parameters are paragraphs that match the regex variable format

reviews = [result.text for result in results] # reviews is an array of the text written in the comments of the webpage 
                                              # found by iterating a loop over the results variable and applying the .text method on each element 
reviews
# note : different websites have different structures so keep that in mind before implementing the webscraping as the code will require relevant changes

"""**STEP 04: Feeding to dataframe and calculating all reviews**"""

import numpy as np
import pandas as pd

df = pd.DataFrame(np.array(reviews),columns = ['Review']) # creating a dataframe that has all the reviews stored as a np array and the column name as reviews
# we further use the df.head and df.tail() methods to inspect hte first and last 5 entries of the df
df

def sentiment_scorer(review):
  tokens = tokenizer.encode(review , return_tensors ='pt' )
  result = model(tokens)
  return int(torch.argmax(result.logits))+1
# this function takes a review ; applies tokenizer and breaks it to smaller parts ; applies the bert model to the tokens
# returns the sentiment for that review

df['Sentiment']= df['Review'].apply(lambda x :sentiment_scorer(x[:512])) # creates a new sentiment column in the dataframe that has
# we take each review , use apply method to apply ; lambda function to run for every review and apply the sentiment scorer function on it to generate the sentiment value 
# the x[:512] is required because this specific model can only do tokenizing and the following procedures on the first 512 characters only

df

